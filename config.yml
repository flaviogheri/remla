# config.yml
data_paths:
  training_file: "data/raw/train_raw.txt"
  test_file: "data/raw/test_raw.txt"
  val_file: "data/raw/val_raw.txt"

processed_paths:
  train: "data/processed/train"
  test: "data/processed/test"
  val: "data/processed/val"

params:
  sequence_length: 200
  loss_function: "binary_crossentropy"
  optimizer: "adam"
  batch_train: 5000
  batch_test: 5000
  categories:
    - "phishing"
    - "legitimate"
  char_index: null  # This will be updated after initializing the tokenizer
  epoch: 30
  embedding_dimension: 50
  dataset_dir: "../dataset/small_dataset/"